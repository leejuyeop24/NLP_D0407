{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5db9310",
   "metadata": {},
   "source": [
    "#### [ 자연어 전처리 - 토큰화 ]\n",
    "- 의미있는 단위로 나눈 작업 ==> 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae507859",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2302b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 패키지의 토큰화 모듈 로딩\n",
    "from nltk.tokenize import *\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a94428",
   "metadata": {},
   "source": [
    "[2] 단어 단위 토큰화 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85be3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장 데이터\n",
    "txt = \"Happy, New Year! Don't stop. Dh.Park!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c28b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 단위 토큰화 : 구두점을 무조건 분리\n",
    "## - 함수\n",
    "result = wordpunct_tokenize(txt)\n",
    "\n",
    "## - 객체 생성\n",
    "wpTokenize = WordPunctTokenizer()\n",
    "result2 = wpTokenize.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc061994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result \n",
      "['Happy', ',', 'New', 'Year', '!', 'Don', \"'\", 't', 'stop', '.', 'Dh', '.', 'Park', '!']\n"
     ]
    }
   ],
   "source": [
    "## 결과 출력 => Don't ==> Don ' t\n",
    "print(f'result2 \\n{result2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b432532",
   "metadata": {},
   "source": [
    "- word-tokenize()함수의 실제 구동 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20216a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happy', ',', 'New', 'Year', '!', 'Do', \"n't\", 'stop.', 'Dh.Park', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Penn Treebank Tokenization\n",
    "## - 표준으로 쓰이고 있는 토큰화 방법 중 하나\n",
    "## - 규칙 1. 하이푼으로 구성된 단어는 하나로 유지\n",
    "## - 규칙 2. 아포스트로피로 접어가 함께 하는 단어는 분리\n",
    "## - word_tokenize()함수 내부에서 호출됨\n",
    "tbTokenizer = TreebankWordTokenizer()\n",
    "tbTokenizer.tokenize(txt) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
