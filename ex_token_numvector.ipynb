{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 자연어 전처리 - 정제+토큰화 ]\n",
    "- 정제 단계 (Cleaning Step)\n",
    "- 토큰화 단계 (Tokenize Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토큰화 관련 모듈\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 파일\n",
    "DATA_FILE   = '../data/test_data.txt'           ## 말뭉치 즉 코퍼스(Copous)     \n",
    "STOP_WORD   = stopwords.words('english')        ## 불용어 즉, 분석에 의미없는 단어들\n",
    "PUNCTUATION = punctuation                       ## 구두점\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORD   : 198개\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "PUNCTUATION : 32개\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(f'STOP_WORD   : {len(STOP_WORD)}개\\n{STOP_WORD}')\n",
    "print(f'PUNCTUATION : {len(PUNCTUATION)}개\\n{PUNCTUATION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 텍스트 데이터 정제<hr>\n",
    "- 영어 : 대소문자 일치\n",
    "- 필요없는 기호, 문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileData==> 1851개\n"
     ]
    }
   ],
   "source": [
    "## - 파일 데이터 읽어오기\n",
    "with open(DATA_FILE, mode='r', encoding='utf-8') as f:\n",
    "    fileData=f.read()\n",
    "\n",
    "##- 확인\n",
    "print(f'fileData==> {len(fileData)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileData==>\n",
      " what a merry-go-round is the e\n"
     ]
    }
   ],
   "source": [
    "## 대소문자 일치 ==> 소문자\n",
    "fileData=fileData.lower()\n",
    "\n",
    "##- 확인\n",
    "print(f'fileData==>\\n {fileData[:30]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 문장 단위 토큰화 + 불용어 제거 + 원형 복원 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what a merry-go-round is the eighteenth collection by british fashion designer alexander mcqueen, made for the autumn/winter 2001 season of his fashion house alexander mcqueen.',\n",
       " \"the collection drew on imagery of clowns and carnivals, inspired by mcqueen's feelings about childhood and his experiences in the fashion industry.\",\n",
       " 'the designs were influenced by military chic, cinema such as nosferatu (1922) and cabaret (1972), 1920s flapper fashion, and the french revolution.',\n",
       " 'the palette comprised dark colours complemented with neutrals and muted greens.',\n",
       " 'the show marked the first appearance of the skull motif that became a signature of the brand.',\n",
       " \"the collection's runway show was staged on 21 february 2001 at the gatliff road warehouse in london, as part of london fashion week.\",\n",
       " \"it was mcqueen's final show in london; all his future collections were presented in paris.\",\n",
       " 'sixty-two looks were presented in the main runway show, with at least six more in the finale.',\n",
       " '[a] the show was staged in a dark room with a carousel at the centre.',\n",
       " 'during the finale, the lights came up to reveal piles of discarded childhood bric-à-brac at the rear of the stage, while models dressed as evil clowns cavorted around the stage, posing in their eveningwear.',\n",
       " 'critical response to the collection was generally positive, and it has attracted some academic analysis for the theme and messaging.',\n",
       " \"like mcqueen's previous show voss (spring/summer 2001), merry-go-round served as a critique of the fashion industry, which he sometimes described as toxic and suffocating.\",\n",
       " 'it contained elements that several authors have taken as references to french luxury goods conglomerate lvmh and its management, with whom mcqueen had a turbulent relationship.',\n",
       " 'ensembles from merry-go-round have appeared in exhibitions such as the mcqueen retrospective alexander mcqueen: savage beauty.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [3-1] 문장 분리 후 문장 단위에서 토큰 분리\n",
    "sentences = sent_tokenize(fileData)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------------------\n",
    "## 함수기능 : 품사 기반 불필요 품사 제거 + 원형 복원 토큰 반환 \n",
    "## 함수이름 : covertOriginal\n",
    "## 매개변수 : pos_token_list - (단어, 품사)형태의 토큰 리스트\n",
    "## 함수결과 : 불필요 품사 제거 + 원형 복원 토큰 리스트\n",
    "## --------------------------------------------------------------\n",
    "def covertOriginal(pos_token_list):\n",
    "    ## 표제어 추출 인스턴스\n",
    "    wnLemma = WordNetLemmatizer()\n",
    "\n",
    "    ## 원형 복원 저장 \n",
    "    result =[]\n",
    "    print(pos_token_list)\n",
    "    ## 형용사, 동사 경우 표제어 즉, 원형 복원\n",
    "    for word, pos in pos_token_list:\n",
    "            \n",
    "        ## 형용사, 동사 경우 표제어 즉, 원형 복원\n",
    "        if pos[:2] in ['JJ', 'VB']:\n",
    "            result.append(wnLemma.lemmatize(word, 'a' if pos=='JJ' else 'v' ))\n",
    "        elif pos not in ['DT', 'IN', 'CD', 'CC']:\n",
    "            ## 토큰 합치기 => 불필요한 품사 제거한 토큰들 그대로 추가\n",
    "            result.append(word)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pos_token_list] ===> [('what', 'WP'), ('a', 'DT'), ('merrygoround', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('eighteenth', 'JJ'), ('collection', 'NN'), ('by', 'IN'), ('british', 'JJ'), ('fashion', 'NN'), ('designer', 'NN'), ('alexander', 'NN'), ('mcqueen', 'NN'), ('made', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('autumnwinter', 'NN'), ('2001', 'CD'), ('season', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('fashion', 'NN'), ('house', 'NN'), ('alexander', 'NN'), ('mcqueen', 'NN')]\n",
      "[('what', 'WP'), ('a', 'DT'), ('merrygoround', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('eighteenth', 'JJ'), ('collection', 'NN'), ('by', 'IN'), ('british', 'JJ'), ('fashion', 'NN'), ('designer', 'NN'), ('alexander', 'NN'), ('mcqueen', 'NN'), ('made', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('autumnwinter', 'NN'), ('2001', 'CD'), ('season', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('fashion', 'NN'), ('house', 'NN'), ('alexander', 'NN'), ('mcqueen', 'NN')]\n",
      "[tokens] ===> ['what', 'merrygoround', 'be', 'eighteenth', 'collection', 'british', 'fashion', 'designer', 'alexander', 'mcqueen', 'make', 'autumnwinter', 'season', 'his', 'fashion', 'house', 'alexander', 'mcqueen']\n",
      "[pos_token_list] ===> [('the', 'DT'), ('collection', 'NN'), ('drew', 'VBD'), ('on', 'IN'), ('imagery', 'NN'), ('of', 'IN'), ('clowns', 'NNS'), ('and', 'CC'), ('carnivals', 'NNS'), ('inspired', 'VBN'), ('by', 'IN'), ('mcqueens', 'NNS'), ('feelings', 'NNS'), ('about', 'IN'), ('childhood', 'NN'), ('and', 'CC'), ('his', 'PRP$'), ('experiences', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('fashion', 'NN'), ('industry', 'NN')]\n",
      "[('the', 'DT'), ('collection', 'NN'), ('drew', 'VBD'), ('on', 'IN'), ('imagery', 'NN'), ('of', 'IN'), ('clowns', 'NNS'), ('and', 'CC'), ('carnivals', 'NNS'), ('inspired', 'VBN'), ('by', 'IN'), ('mcqueens', 'NNS'), ('feelings', 'NNS'), ('about', 'IN'), ('childhood', 'NN'), ('and', 'CC'), ('his', 'PRP$'), ('experiences', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('fashion', 'NN'), ('industry', 'NN')]\n",
      "[tokens] ===> ['collection', 'draw', 'imagery', 'clowns', 'carnivals', 'inspire', 'mcqueens', 'feelings', 'childhood', 'his', 'experiences', 'fashion', 'industry']\n",
      "[pos_token_list] ===> [('the', 'DT'), ('designs', 'NNS'), ('were', 'VBD'), ('influenced', 'VBN'), ('by', 'IN'), ('military', 'JJ'), ('chic', 'JJ'), ('cinema', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('nosferatu', 'JJ'), ('1922', 'CD'), ('and', 'CC'), ('cabaret', 'VB'), ('1972', 'CD'), ('1920s', 'CD'), ('flapper', 'JJ'), ('fashion', 'NN'), ('and', 'CC'), ('the', 'DT'), ('french', 'JJ'), ('revolution', 'NN')]\n",
      "[('the', 'DT'), ('designs', 'NNS'), ('were', 'VBD'), ('influenced', 'VBN'), ('by', 'IN'), ('military', 'JJ'), ('chic', 'JJ'), ('cinema', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('nosferatu', 'JJ'), ('1922', 'CD'), ('and', 'CC'), ('cabaret', 'VB'), ('1972', 'CD'), ('1920s', 'CD'), ('flapper', 'JJ'), ('fashion', 'NN'), ('and', 'CC'), ('the', 'DT'), ('french', 'JJ'), ('revolution', 'NN')]\n",
      "[tokens] ===> ['designs', 'be', 'influence', 'military', 'chic', 'cinema', 'such', 'nosferatu', 'cabaret', 'flapper', 'fashion', 'french', 'revolution']\n",
      "[pos_token_list] ===> [('the', 'DT'), ('palette', 'NN'), ('comprised', 'VBD'), ('dark', 'JJ'), ('colours', 'NNS'), ('complemented', 'VBN'), ('with', 'IN'), ('neutrals', 'NNS'), ('and', 'CC'), ('muted', 'VBD'), ('greens', 'NNS')]\n",
      "[('the', 'DT'), ('palette', 'NN'), ('comprised', 'VBD'), ('dark', 'JJ'), ('colours', 'NNS'), ('complemented', 'VBN'), ('with', 'IN'), ('neutrals', 'NNS'), ('and', 'CC'), ('muted', 'VBD'), ('greens', 'NNS')]\n",
      "[tokens] ===> ['palette', 'comprise', 'dark', 'colours', 'complement', 'neutrals', 'mute', 'greens']\n",
      "[pos_token_list] ===> [('the', 'DT'), ('show', 'NN'), ('marked', 'VBD'), ('the', 'DT'), ('first', 'JJ'), ('appearance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('skull', 'NN'), ('motif', 'NN'), ('that', 'WDT'), ('became', 'VBD'), ('a', 'DT'), ('signature', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brand', 'NN')]\n",
      "[('the', 'DT'), ('show', 'NN'), ('marked', 'VBD'), ('the', 'DT'), ('first', 'JJ'), ('appearance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('skull', 'NN'), ('motif', 'NN'), ('that', 'WDT'), ('became', 'VBD'), ('a', 'DT'), ('signature', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brand', 'NN')]\n",
      "[tokens] ===> ['show', 'mark', 'first', 'appearance', 'skull', 'motif', 'that', 'become', 'signature', 'brand']\n",
      "[pos_token_list] ===> [('the', 'DT'), ('collections', 'NNS'), ('runway', 'VBP'), ('show', 'NN'), ('was', 'VBD'), ('staged', 'VBN'), ('on', 'IN'), ('21', 'CD'), ('february', 'JJ'), ('2001', 'CD'), ('at', 'IN'), ('the', 'DT'), ('gatliff', 'NN'), ('road', 'NN'), ('warehouse', 'NN'), ('in', 'IN'), ('london', 'NN'), ('as', 'IN'), ('part', 'NN'), ('of', 'IN'), ('london', 'JJ'), ('fashion', 'NN'), ('week', 'NN')]\n",
      "[('the', 'DT'), ('collections', 'NNS'), ('runway', 'VBP'), ('show', 'NN'), ('was', 'VBD'), ('staged', 'VBN'), ('on', 'IN'), ('21', 'CD'), ('february', 'JJ'), ('2001', 'CD'), ('at', 'IN'), ('the', 'DT'), ('gatliff', 'NN'), ('road', 'NN'), ('warehouse', 'NN'), ('in', 'IN'), ('london', 'NN'), ('as', 'IN'), ('part', 'NN'), ('of', 'IN'), ('london', 'JJ'), ('fashion', 'NN'), ('week', 'NN')]\n",
      "[tokens] ===> ['collections', 'runway', 'show', 'be', 'stag', 'february', 'gatliff', 'road', 'warehouse', 'london', 'part', 'london', 'fashion', 'week']\n",
      "[pos_token_list] ===> [('it', 'PRP'), ('was', 'VBD'), ('mcqueens', 'JJ'), ('final', 'JJ'), ('show', 'NN'), ('in', 'IN'), ('london', 'NN'), ('all', 'PDT'), ('his', 'PRP$'), ('future', 'JJ'), ('collections', 'NNS'), ('were', 'VBD'), ('presented', 'VBN'), ('in', 'IN'), ('paris', 'NN')]\n",
      "[('it', 'PRP'), ('was', 'VBD'), ('mcqueens', 'JJ'), ('final', 'JJ'), ('show', 'NN'), ('in', 'IN'), ('london', 'NN'), ('all', 'PDT'), ('his', 'PRP$'), ('future', 'JJ'), ('collections', 'NNS'), ('were', 'VBD'), ('presented', 'VBN'), ('in', 'IN'), ('paris', 'NN')]\n",
      "[tokens] ===> ['it', 'be', 'mcqueens', 'final', 'show', 'london', 'all', 'his', 'future', 'collections', 'be', 'present', 'paris']\n",
      "[pos_token_list] ===> [('sixtytwo', 'JJ'), ('looks', 'NNS'), ('were', 'VBD'), ('presented', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('main', 'JJ'), ('runway', 'NN'), ('show', 'NN'), ('with', 'IN'), ('at', 'IN'), ('least', 'JJS'), ('six', 'CD'), ('more', 'JJR'), ('in', 'IN'), ('the', 'DT'), ('finale', 'NN')]\n",
      "[('sixtytwo', 'JJ'), ('looks', 'NNS'), ('were', 'VBD'), ('presented', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('main', 'JJ'), ('runway', 'NN'), ('show', 'NN'), ('with', 'IN'), ('at', 'IN'), ('least', 'JJS'), ('six', 'CD'), ('more', 'JJR'), ('in', 'IN'), ('the', 'DT'), ('finale', 'NN')]\n",
      "[tokens] ===> ['sixtytwo', 'looks', 'be', 'present', 'main', 'runway', 'show', 'least', 'more', 'finale']\n",
      "[pos_token_list] ===> [('a', 'DT'), ('the', 'DT'), ('show', 'NN'), ('was', 'VBD'), ('staged', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('dark', 'JJ'), ('room', 'NN'), ('with', 'IN'), ('a', 'DT'), ('carousel', 'NN'), ('at', 'IN'), ('the', 'DT'), ('centre', 'NN')]\n",
      "[('a', 'DT'), ('the', 'DT'), ('show', 'NN'), ('was', 'VBD'), ('staged', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('dark', 'JJ'), ('room', 'NN'), ('with', 'IN'), ('a', 'DT'), ('carousel', 'NN'), ('at', 'IN'), ('the', 'DT'), ('centre', 'NN')]\n",
      "[tokens] ===> ['show', 'be', 'stag', 'dark', 'room', 'carousel', 'centre']\n",
      "[pos_token_list] ===> [('during', 'IN'), ('the', 'DT'), ('finale', 'NN'), ('the', 'DT'), ('lights', 'NNS'), ('came', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('reveal', 'VB'), ('piles', 'NNS'), ('of', 'IN'), ('discarded', 'JJ'), ('childhood', 'NN'), ('bricàbrac', 'NN'), ('at', 'IN'), ('the', 'DT'), ('rear', 'NN'), ('of', 'IN'), ('the', 'DT'), ('stage', 'NN'), ('while', 'IN'), ('models', 'NNS'), ('dressed', 'VBN'), ('as', 'IN'), ('evil', 'JJ'), ('clowns', 'NNS'), ('cavorted', 'VBN'), ('around', 'IN'), ('the', 'DT'), ('stage', 'NN'), ('posing', 'VBG'), ('in', 'IN'), ('their', 'PRP$'), ('eveningwear', 'NN')]\n",
      "[('during', 'IN'), ('the', 'DT'), ('finale', 'NN'), ('the', 'DT'), ('lights', 'NNS'), ('came', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('reveal', 'VB'), ('piles', 'NNS'), ('of', 'IN'), ('discarded', 'JJ'), ('childhood', 'NN'), ('bricàbrac', 'NN'), ('at', 'IN'), ('the', 'DT'), ('rear', 'NN'), ('of', 'IN'), ('the', 'DT'), ('stage', 'NN'), ('while', 'IN'), ('models', 'NNS'), ('dressed', 'VBN'), ('as', 'IN'), ('evil', 'JJ'), ('clowns', 'NNS'), ('cavorted', 'VBN'), ('around', 'IN'), ('the', 'DT'), ('stage', 'NN'), ('posing', 'VBG'), ('in', 'IN'), ('their', 'PRP$'), ('eveningwear', 'NN')]\n",
      "[tokens] ===> ['finale', 'lights', 'come', 'up', 'to', 'reveal', 'piles', 'discarded', 'childhood', 'bricàbrac', 'rear', 'stage', 'models', 'dress', 'evil', 'clowns', 'cavort', 'stage', 'pose', 'their', 'eveningwear']\n",
      "[pos_token_list] ===> [('critical', 'JJ'), ('response', 'NN'), ('to', 'TO'), ('the', 'DT'), ('collection', 'NN'), ('was', 'VBD'), ('generally', 'RB'), ('positive', 'JJ'), ('and', 'CC'), ('it', 'PRP'), ('has', 'VBZ'), ('attracted', 'VBN'), ('some', 'DT'), ('academic', 'JJ'), ('analysis', 'NN'), ('for', 'IN'), ('the', 'DT'), ('theme', 'NN'), ('and', 'CC'), ('messaging', 'NN')]\n",
      "[('critical', 'JJ'), ('response', 'NN'), ('to', 'TO'), ('the', 'DT'), ('collection', 'NN'), ('was', 'VBD'), ('generally', 'RB'), ('positive', 'JJ'), ('and', 'CC'), ('it', 'PRP'), ('has', 'VBZ'), ('attracted', 'VBN'), ('some', 'DT'), ('academic', 'JJ'), ('analysis', 'NN'), ('for', 'IN'), ('the', 'DT'), ('theme', 'NN'), ('and', 'CC'), ('messaging', 'NN')]\n",
      "[tokens] ===> ['critical', 'response', 'to', 'collection', 'be', 'generally', 'positive', 'it', 'have', 'attract', 'academic', 'analysis', 'theme', 'messaging']\n",
      "[pos_token_list] ===> [('like', 'IN'), ('mcqueens', 'NNS'), ('previous', 'JJ'), ('show', 'NN'), ('voss', 'IN'), ('springsummer', 'NN'), ('2001', 'CD'), ('merrygoround', 'NN'), ('served', 'VBD'), ('as', 'IN'), ('a', 'DT'), ('critique', 'NN'), ('of', 'IN'), ('the', 'DT'), ('fashion', 'NN'), ('industry', 'NN'), ('which', 'WDT'), ('he', 'PRP'), ('sometimes', 'RB'), ('described', 'VBD'), ('as', 'IN'), ('toxic', 'NN'), ('and', 'CC'), ('suffocating', 'NN')]\n",
      "[('like', 'IN'), ('mcqueens', 'NNS'), ('previous', 'JJ'), ('show', 'NN'), ('voss', 'IN'), ('springsummer', 'NN'), ('2001', 'CD'), ('merrygoround', 'NN'), ('served', 'VBD'), ('as', 'IN'), ('a', 'DT'), ('critique', 'NN'), ('of', 'IN'), ('the', 'DT'), ('fashion', 'NN'), ('industry', 'NN'), ('which', 'WDT'), ('he', 'PRP'), ('sometimes', 'RB'), ('described', 'VBD'), ('as', 'IN'), ('toxic', 'NN'), ('and', 'CC'), ('suffocating', 'NN')]\n",
      "[tokens] ===> ['mcqueens', 'previous', 'show', 'springsummer', 'merrygoround', 'serve', 'critique', 'fashion', 'industry', 'which', 'he', 'sometimes', 'describe', 'toxic', 'suffocating']\n",
      "[pos_token_list] ===> [('it', 'PRP'), ('contained', 'VBD'), ('elements', 'NNS'), ('that', 'IN'), ('several', 'JJ'), ('authors', 'NNS'), ('have', 'VBP'), ('taken', 'VBN'), ('as', 'IN'), ('references', 'NNS'), ('to', 'TO'), ('french', 'JJ'), ('luxury', 'NN'), ('goods', 'NNS'), ('conglomerate', 'VBP'), ('lvmh', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('management', 'NN'), ('with', 'IN'), ('whom', 'WP'), ('mcqueen', 'NN'), ('had', 'VBD'), ('a', 'DT'), ('turbulent', 'NN'), ('relationship', 'NN')]\n",
      "[('it', 'PRP'), ('contained', 'VBD'), ('elements', 'NNS'), ('that', 'IN'), ('several', 'JJ'), ('authors', 'NNS'), ('have', 'VBP'), ('taken', 'VBN'), ('as', 'IN'), ('references', 'NNS'), ('to', 'TO'), ('french', 'JJ'), ('luxury', 'NN'), ('goods', 'NNS'), ('conglomerate', 'VBP'), ('lvmh', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('management', 'NN'), ('with', 'IN'), ('whom', 'WP'), ('mcqueen', 'NN'), ('had', 'VBD'), ('a', 'DT'), ('turbulent', 'NN'), ('relationship', 'NN')]\n",
      "[tokens] ===> ['it', 'contain', 'elements', 'several', 'authors', 'have', 'take', 'references', 'to', 'french', 'luxury', 'goods', 'conglomerate', 'lvmh', 'its', 'management', 'whom', 'mcqueen', 'have', 'turbulent', 'relationship']\n",
      "[pos_token_list] ===> [('ensembles', 'NNS'), ('from', 'IN'), ('merrygoround', 'NN'), ('have', 'VBP'), ('appeared', 'VBN'), ('in', 'IN'), ('exhibitions', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('mcqueen', 'JJ'), ('retrospective', 'JJ'), ('alexander', 'NN'), ('mcqueen', 'JJ'), ('savage', 'NN'), ('beauty', 'NN')]\n",
      "[('ensembles', 'NNS'), ('from', 'IN'), ('merrygoround', 'NN'), ('have', 'VBP'), ('appeared', 'VBN'), ('in', 'IN'), ('exhibitions', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('mcqueen', 'JJ'), ('retrospective', 'JJ'), ('alexander', 'NN'), ('mcqueen', 'JJ'), ('savage', 'NN'), ('beauty', 'NN')]\n",
      "[tokens] ===> ['ensembles', 'merrygoround', 'have', 'appear', 'exhibitions', 'such', 'mcqueen', 'retrospective', 'alexander', 'mcqueen', 'savage', 'beauty']\n",
      "총 문장 수 : 14개\n",
      "[['merrygoround', 'eighteenth', 'collection', 'british', 'fashion', 'designer', 'alexander', 'mcqueen', 'make', 'autumnwinter', 'season', 'fashion', 'house', 'alexander', 'mcqueen'], ['collection', 'draw', 'imagery', 'clowns', 'carnivals', 'inspire', 'mcqueens', 'feelings', 'childhood', 'experiences', 'fashion', 'industry'], ['designs', 'influence', 'military', 'chic', 'cinema', 'nosferatu', 'cabaret', 'flapper', 'fashion', 'french', 'revolution'], ['palette', 'comprise', 'dark', 'colours', 'complement', 'neutrals', 'mute', 'greens'], ['show', 'mark', 'first', 'appearance', 'skull', 'motif', 'become', 'signature', 'brand'], ['collections', 'runway', 'show', 'stag', 'february', 'gatliff', 'road', 'warehouse', 'london', 'part', 'london', 'fashion', 'week'], ['mcqueens', 'final', 'show', 'london', 'future', 'collections', 'present', 'paris'], ['sixtytwo', 'looks', 'present', 'main', 'runway', 'show', 'least', 'finale'], ['show', 'stag', 'dark', 'room', 'carousel', 'centre'], ['finale', 'lights', 'come', 'reveal', 'piles', 'discarded', 'childhood', 'bricàbrac', 'rear', 'stage', 'models', 'dress', 'evil', 'clowns', 'cavort', 'stage', 'pose', 'eveningwear'], ['critical', 'response', 'collection', 'generally', 'positive', 'attract', 'academic', 'analysis', 'theme', 'messaging'], ['mcqueens', 'previous', 'show', 'springsummer', 'merrygoround', 'serve', 'critique', 'fashion', 'industry', 'sometimes', 'describe', 'toxic', 'suffocating'], ['contain', 'elements', 'several', 'authors', 'take', 'references', 'french', 'luxury', 'goods', 'conglomerate', 'lvmh', 'management', 'mcqueen', 'turbulent', 'relationship'], ['ensembles', 'merrygoround', 'appear', 'exhibitions', 'mcqueen', 'retrospective', 'alexander', 'mcqueen', 'savage', 'beauty']]\n"
     ]
    }
   ],
   "source": [
    "## [3-2] 문장 단위 전처리 된 문장 리스트 추출 \n",
    "\n",
    "## - 문장별 토큰 리스트, 품사 토큰 리스트 저장 \n",
    "sent_token_list, pos_token_list =[],[]\n",
    "\n",
    "## - 문장별 토큰화 처리\n",
    "for sent in sentences:\n",
    "    ## 문장 단위로 구두점 제거, 토큰 분리\n",
    "    for p in PUNCTUATION: \n",
    "        if p in sent: sent=sent.replace(p,'')\n",
    "    \n",
    "    ## 토큰 분리=> list 반환\n",
    "    pos_token_list = pos_tag( word_tokenize(sent) )\n",
    "    print(f'[pos_token_list] ===> {pos_token_list}')\n",
    "\n",
    "    ## 형용사 'JJ'=> 'a', 동사 'VB' => 'v'원형 복원 \n",
    "    ## 불필요한 품사 제거한 토큰들 저장\n",
    "    tokens = covertOriginal(pos_token_list)\n",
    "    print(f'[tokens] ===> {tokens}')\n",
    "\n",
    "    ## 토큰 리스트에서 불용어 제거\n",
    "    words = [ w for w in tokens  if w not in STOP_WORD ]\n",
    "\n",
    "    ## 문장 단위 토큰 저장 \n",
    "    sent_token_list.append(words)\n",
    "    \n",
    "print(F'총 문장 수 : {len(sent_token_list)}개\\n{sent_token_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 단어 사전 생성 <hr>\n",
    "- 토큰 -- 정수 인코딩\n",
    "- 특수토큰 : 없는 토큰 UNK, 길이 맞춤용 토큰 PAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4-1) 중복 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_list => ['merrygoround', 'eighteenth', 'collection', 'british', 'fashion', 'designer', 'alexander', 'mcqueen', 'make', 'autumnwinter', 'season', 'fashion', 'house', 'alexander', 'mcqueen', 'collection', 'draw', 'imagery', 'clowns', 'carnivals', 'inspire', 'mcqueens', 'feelings', 'childhood', 'experiences', 'fashion', 'industry', 'designs', 'influence', 'military', 'chic', 'cinema', 'nosferatu', 'cabaret', 'flapper', 'fashion', 'french', 'revolution', 'palette', 'comprise', 'dark', 'colours', 'complement', 'neutrals', 'mute', 'greens', 'show', 'mark', 'first', 'appearance', 'skull', 'motif', 'become', 'signature', 'brand', 'collections', 'runway', 'show', 'stag', 'february', 'gatliff', 'road', 'warehouse', 'london', 'part', 'london', 'fashion', 'week', 'mcqueens', 'final', 'show', 'london', 'future', 'collections', 'present', 'paris', 'sixtytwo', 'looks', 'present', 'main', 'runway', 'show', 'least', 'finale', 'show', 'stag', 'dark', 'room', 'carousel', 'centre', 'finale', 'lights', 'come', 'reveal', 'piles', 'discarded', 'childhood', 'bricàbrac', 'rear', 'stage', 'models', 'dress', 'evil', 'clowns', 'cavort', 'stage', 'pose', 'eveningwear', 'critical', 'response', 'collection', 'generally', 'positive', 'attract', 'academic', 'analysis', 'theme', 'messaging', 'mcqueens', 'previous', 'show', 'springsummer', 'merrygoround', 'serve', 'critique', 'fashion', 'industry', 'sometimes', 'describe', 'toxic', 'suffocating', 'contain', 'elements', 'several', 'authors', 'take', 'references', 'french', 'luxury', 'goods', 'conglomerate', 'lvmh', 'management', 'mcqueen', 'turbulent', 'relationship', 'ensembles', 'merrygoround', 'appear', 'exhibitions', 'mcqueen', 'retrospective', 'alexander', 'mcqueen', 'savage', 'beauty']\n",
      "token_list => 156개\n",
      "token_list => ['attract', 'toxic', 'cabaret', 'imagery', 'ensembles', 'become', 'collections', 'skull', 'rear', 'elements', 'conglomerate', 'models', 'looks', 'childhood', 'stage', 'springsummer', 'greens', 'room', 'autumnwinter', 'colours', 'generally', 'mcqueens', 'sixtytwo', 'pose', 'comprise', 'suffocating', 'appearance', 'runway', 'discarded', 'nosferatu', 'exhibitions', 'fashion', 'eveningwear', 'complement', 'evil', 'serve', 'messaging', 'positive', 'reveal', 'piles', 'stag', 'influence', 'house', 'mark', 'french', 'neutrals', 'road', 'appear', 'theme', 'beauty', 'revolution', 'part', 'warehouse', 'flapper', 'turbulent', 'authors', 'clowns', 'come', 'palette', 'experiences', 'merrygoround', 'academic', 'carnivals', 'finale', 'mute', 'future', 'february', 'alexander', 'savage', 'least', 'sometimes', 'luxury', 'critique', 'relationship', 'paris', 'critical', 'dark', 'season', 'present', 'week', 'make', 'designs', 'eighteenth', 'dress', 'draw', 'first', 'signature', 'carousel', 'military', 'centre', 'final', 'lights', 'industry', 'london', 'british', 'several', 'previous', 'references', 'inspire', 'collection', 'chic', 'take', 'cinema', 'retrospective', 'bricàbrac', 'show', 'goods', 'mcqueen', 'gatliff', 'analysis', 'lvmh', 'feelings', 'designer', 'management', 'contain', 'response', 'cavort', 'describe', 'motif', 'brand', 'main']\n",
      "token_list => 121개\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## - 문장의 토큰들 하나로 합치기\n",
    "token_list = [ word  for sent in sent_token_list  for word in sent  ]\n",
    "\n",
    "print(f'token_list => {token_list}')\n",
    "print(f'token_list => {len(token_list)}개')\n",
    "\n",
    "## - 토큰들 중복 제거\n",
    "token_list = list(set(token_list))\n",
    "print(f'token_list => {token_list}')\n",
    "print(f'token_list => {len(token_list)}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4-2) 단어 사전 생성<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0, '<UNK>': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### dict 타입으로 단어사전 생성\n",
    "VOCAB_TO_IDX = {'<PAD>':0, '<UNK>':1 }\n",
    "VOCAB_TO_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토큰들에게 정수 숫자 부여\n",
    "for idx, token in enumerate(token_list, 2):\n",
    "    VOCAB_TO_IDX[token]=idx \n",
    "\n",
    "## idx => word로 변환\n",
    "IDX_TO_TOKEN = {v:k for k,v in VOCAB_TO_IDX.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_TO_IDX=>\n",
      " {'<PAD>': 0, '<UNK>': 1, 'attract': 2, 'toxic': 3, 'cabaret': 4, 'imagery': 5, 'ensembles': 6, 'become': 7, 'collections': 8, 'skull': 9, 'rear': 10, 'elements': 11, 'conglomerate': 12, 'models': 13, 'looks': 14, 'childhood': 15, 'stage': 16, 'springsummer': 17, 'greens': 18, 'room': 19, 'autumnwinter': 20, 'colours': 21, 'generally': 22, 'mcqueens': 23, 'sixtytwo': 24, 'pose': 25, 'comprise': 26, 'suffocating': 27, 'appearance': 28, 'runway': 29, 'discarded': 30, 'nosferatu': 31, 'exhibitions': 32, 'fashion': 33, 'eveningwear': 34, 'complement': 35, 'evil': 36, 'serve': 37, 'messaging': 38, 'positive': 39, 'reveal': 40, 'piles': 41, 'stag': 42, 'influence': 43, 'house': 44, 'mark': 45, 'french': 46, 'neutrals': 47, 'road': 48, 'appear': 49, 'theme': 50, 'beauty': 51, 'revolution': 52, 'part': 53, 'warehouse': 54, 'flapper': 55, 'turbulent': 56, 'authors': 57, 'clowns': 58, 'come': 59, 'palette': 60, 'experiences': 61, 'merrygoround': 62, 'academic': 63, 'carnivals': 64, 'finale': 65, 'mute': 66, 'future': 67, 'february': 68, 'alexander': 69, 'savage': 70, 'least': 71, 'sometimes': 72, 'luxury': 73, 'critique': 74, 'relationship': 75, 'paris': 76, 'critical': 77, 'dark': 78, 'season': 79, 'present': 80, 'week': 81, 'make': 82, 'designs': 83, 'eighteenth': 84, 'dress': 85, 'draw': 86, 'first': 87, 'signature': 88, 'carousel': 89, 'military': 90, 'centre': 91, 'final': 92, 'lights': 93, 'industry': 94, 'london': 95, 'british': 96, 'several': 97, 'previous': 98, 'references': 99, 'inspire': 100, 'collection': 101, 'chic': 102, 'take': 103, 'cinema': 104, 'retrospective': 105, 'bricàbrac': 106, 'show': 107, 'goods': 108, 'mcqueen': 109, 'gatliff': 110, 'analysis': 111, 'lvmh': 112, 'feelings': 113, 'designer': 114, 'management': 115, 'contain': 116, 'response': 117, 'cavort': 118, 'describe': 119, 'motif': 120, 'brand': 121, 'main': 122}\n",
      "IDX_TO_TOKEN=>\n",
      " {0: '<PAD>', 1: '<UNK>', 2: 'attract', 3: 'toxic', 4: 'cabaret', 5: 'imagery', 6: 'ensembles', 7: 'become', 8: 'collections', 9: 'skull', 10: 'rear', 11: 'elements', 12: 'conglomerate', 13: 'models', 14: 'looks', 15: 'childhood', 16: 'stage', 17: 'springsummer', 18: 'greens', 19: 'room', 20: 'autumnwinter', 21: 'colours', 22: 'generally', 23: 'mcqueens', 24: 'sixtytwo', 25: 'pose', 26: 'comprise', 27: 'suffocating', 28: 'appearance', 29: 'runway', 30: 'discarded', 31: 'nosferatu', 32: 'exhibitions', 33: 'fashion', 34: 'eveningwear', 35: 'complement', 36: 'evil', 37: 'serve', 38: 'messaging', 39: 'positive', 40: 'reveal', 41: 'piles', 42: 'stag', 43: 'influence', 44: 'house', 45: 'mark', 46: 'french', 47: 'neutrals', 48: 'road', 49: 'appear', 50: 'theme', 51: 'beauty', 52: 'revolution', 53: 'part', 54: 'warehouse', 55: 'flapper', 56: 'turbulent', 57: 'authors', 58: 'clowns', 59: 'come', 60: 'palette', 61: 'experiences', 62: 'merrygoround', 63: 'academic', 64: 'carnivals', 65: 'finale', 66: 'mute', 67: 'future', 68: 'february', 69: 'alexander', 70: 'savage', 71: 'least', 72: 'sometimes', 73: 'luxury', 74: 'critique', 75: 'relationship', 76: 'paris', 77: 'critical', 78: 'dark', 79: 'season', 80: 'present', 81: 'week', 82: 'make', 83: 'designs', 84: 'eighteenth', 85: 'dress', 86: 'draw', 87: 'first', 88: 'signature', 89: 'carousel', 90: 'military', 91: 'centre', 92: 'final', 93: 'lights', 94: 'industry', 95: 'london', 96: 'british', 97: 'several', 98: 'previous', 99: 'references', 100: 'inspire', 101: 'collection', 102: 'chic', 103: 'take', 104: 'cinema', 105: 'retrospective', 106: 'bricàbrac', 107: 'show', 108: 'goods', 109: 'mcqueen', 110: 'gatliff', 111: 'analysis', 112: 'lvmh', 113: 'feelings', 114: 'designer', 115: 'management', 116: 'contain', 117: 'response', 118: 'cavort', 119: 'describe', 120: 'motif', 121: 'brand', 122: 'main'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('VOCAB_TO_IDX=>\\n', VOCAB_TO_IDX)\n",
    "print('IDX_TO_TOKEN=>\\n', IDX_TO_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 자연어 => 숫자 변환 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[문장] ['merrygoround', 'eighteenth', 'collection', 'british', 'fashion', 'designer', 'alexander', 'mcqueen', 'make', 'autumnwinter', 'season', 'fashion', 'house', 'alexander', 'mcqueen']\n",
      "[수치] [62, 84, 101, 96, 33, 114, 69, 109, 82, 20, 79, 33, 44, 69, 109]\n",
      "\n",
      "[문장] ['collection', 'draw', 'imagery', 'clowns', 'carnivals', 'inspire', 'mcqueens', 'feelings', 'childhood', 'experiences', 'fashion', 'industry']\n",
      "[수치] [101, 86, 5, 58, 64, 100, 23, 113, 15, 61, 33, 94]\n",
      "\n",
      "[문장] ['designs', 'influence', 'military', 'chic', 'cinema', 'nosferatu', 'cabaret', 'flapper', 'fashion', 'french', 'revolution']\n",
      "[수치] [83, 43, 90, 102, 104, 31, 4, 55, 33, 46, 52]\n",
      "\n",
      "[문장] ['palette', 'comprise', 'dark', 'colours', 'complement', 'neutrals', 'mute', 'greens']\n",
      "[수치] [60, 26, 78, 21, 35, 47, 66, 18]\n",
      "\n",
      "[문장] ['show', 'mark', 'first', 'appearance', 'skull', 'motif', 'become', 'signature', 'brand']\n",
      "[수치] [107, 45, 87, 28, 9, 120, 7, 88, 121]\n",
      "\n",
      "[문장] ['collections', 'runway', 'show', 'stag', 'february', 'gatliff', 'road', 'warehouse', 'london', 'part', 'london', 'fashion', 'week']\n",
      "[수치] [8, 29, 107, 42, 68, 110, 48, 54, 95, 53, 95, 33, 81]\n",
      "\n",
      "[문장] ['mcqueens', 'final', 'show', 'london', 'future', 'collections', 'present', 'paris']\n",
      "[수치] [23, 92, 107, 95, 67, 8, 80, 76]\n",
      "\n",
      "[문장] ['sixtytwo', 'looks', 'present', 'main', 'runway', 'show', 'least', 'finale']\n",
      "[수치] [24, 14, 80, 122, 29, 107, 71, 65]\n",
      "\n",
      "[문장] ['show', 'stag', 'dark', 'room', 'carousel', 'centre']\n",
      "[수치] [107, 42, 78, 19, 89, 91]\n",
      "\n",
      "[문장] ['finale', 'lights', 'come', 'reveal', 'piles', 'discarded', 'childhood', 'bricàbrac', 'rear', 'stage', 'models', 'dress', 'evil', 'clowns', 'cavort', 'stage', 'pose', 'eveningwear']\n",
      "[수치] [65, 93, 59, 40, 41, 30, 15, 106, 10, 16, 13, 85, 36, 58, 118, 16, 25, 34]\n",
      "\n",
      "[문장] ['critical', 'response', 'collection', 'generally', 'positive', 'attract', 'academic', 'analysis', 'theme', 'messaging']\n",
      "[수치] [77, 117, 101, 22, 39, 2, 63, 111, 50, 38]\n",
      "\n",
      "[문장] ['mcqueens', 'previous', 'show', 'springsummer', 'merrygoround', 'serve', 'critique', 'fashion', 'industry', 'sometimes', 'describe', 'toxic', 'suffocating']\n",
      "[수치] [23, 98, 107, 17, 62, 37, 74, 33, 94, 72, 119, 3, 27]\n",
      "\n",
      "[문장] ['contain', 'elements', 'several', 'authors', 'take', 'references', 'french', 'luxury', 'goods', 'conglomerate', 'lvmh', 'management', 'mcqueen', 'turbulent', 'relationship']\n",
      "[수치] [116, 11, 97, 57, 103, 99, 46, 73, 108, 12, 112, 115, 109, 56, 75]\n",
      "\n",
      "[문장] ['ensembles', 'merrygoround', 'appear', 'exhibitions', 'mcqueen', 'retrospective', 'alexander', 'mcqueen', 'savage', 'beauty']\n",
      "[수치] [6, 62, 49, 32, 109, 105, 69, 109, 70, 51]\n"
     ]
    }
   ],
   "source": [
    "## 문장별 토큰 ==> 수치화 \n",
    "SENT_NUM_LIST =[]\n",
    "for sent_line in sent_token_list:\n",
    "    setnums= [ VOCAB_TO_IDX[word] for word in sent_line]\n",
    "    print('\\n[문장]', sent_line)\n",
    "    print('[수치]', setnums)\n",
    "    SENT_NUM_LIST.append(setnums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 84, 101, 96, 33, 114, 69, 109, 82, 20, 79, 33, 44, 69, 109]\n",
      "[101, 86, 5, 58, 64, 100, 23, 113, 15, 61, 33, 94]\n",
      "[83, 43, 90, 102, 104, 31, 4, 55, 33, 46, 52]\n",
      "[60, 26, 78, 21, 35, 47, 66, 18]\n",
      "[107, 45, 87, 28, 9, 120, 7, 88, 121]\n",
      "[8, 29, 107, 42, 68, 110, 48, 54, 95, 53, 95, 33, 81]\n",
      "[23, 92, 107, 95, 67, 8, 80, 76]\n",
      "[24, 14, 80, 122, 29, 107, 71, 65]\n",
      "[107, 42, 78, 19, 89, 91]\n",
      "[65, 93, 59, 40, 41, 30, 15, 106, 10, 16, 13, 85, 36, 58, 118, 16, 25, 34]\n",
      "[77, 117, 101, 22, 39, 2, 63, 111, 50, 38]\n",
      "[23, 98, 107, 17, 62, 37, 74, 33, 94, 72, 119, 3, 27]\n",
      "[116, 11, 97, 57, 103, 99, 46, 73, 108, 12, 112, 115, 109, 56, 75]\n",
      "[6, 62, 49, 32, 109, 105, 69, 109, 70, 51]\n"
     ]
    }
   ],
   "source": [
    "## 숫자 문장들 체크\n",
    "for _ in SENT_NUM_LIST: print( _ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] 패딩 : 모든 문장 길이 맞추기<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장별 단어 개수 :  [15, 12, 11, 8, 9, 13, 8, 8, 6, 18, 10, 13, 15, 10]\n",
      "가장 긴 문장    :  18, 가장 짧은 문장 :6\n"
     ]
    }
   ],
   "source": [
    "## 문자별 단어 갯수 체크\n",
    "length  = [ len(sent) for sent in SENT_NUM_LIST]\n",
    "\n",
    "print(f'문장별 단어 개수 :  {length}')\n",
    "print(f'가장 긴 문장    :  {max(length)}, 가장 짧은 문장 :{min(length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장별 길이 일치 => 가장 긴문장\n",
    "MAX_LEN = max(length)\n",
    "NUMS = len(SENT_NUM_LIST)\n",
    "for idx in range(NUMS):\n",
    "    sent_len = len(SENT_NUM_LIST[idx])\n",
    "    if sent_len != MAX_LEN:\n",
    "        for _ in range(MAX_LEN-sent_len):\n",
    "            SENT_NUM_LIST[idx].append( VOCAB_TO_IDX['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18개 : [62, 84, 101, 96, 33, 114, 69, 109, 82, 20, 79, 33, 44, 69, 109, 0, 0, 0]\n",
      "18개 : [101, 86, 5, 58, 64, 100, 23, 113, 15, 61, 33, 94, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [83, 43, 90, 102, 104, 31, 4, 55, 33, 46, 52, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [60, 26, 78, 21, 35, 47, 66, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [107, 45, 87, 28, 9, 120, 7, 88, 121, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [8, 29, 107, 42, 68, 110, 48, 54, 95, 53, 95, 33, 81, 0, 0, 0, 0, 0]\n",
      "18개 : [23, 92, 107, 95, 67, 8, 80, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [24, 14, 80, 122, 29, 107, 71, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [107, 42, 78, 19, 89, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [65, 93, 59, 40, 41, 30, 15, 106, 10, 16, 13, 85, 36, 58, 118, 16, 25, 34]\n",
      "18개 : [77, 117, 101, 22, 39, 2, 63, 111, 50, 38, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18개 : [23, 98, 107, 17, 62, 37, 74, 33, 94, 72, 119, 3, 27, 0, 0, 0, 0, 0]\n",
      "18개 : [116, 11, 97, 57, 103, 99, 46, 73, 108, 12, 112, 115, 109, 56, 75, 0, 0, 0]\n",
      "18개 : [6, 62, 49, 32, 109, 105, 69, 109, 70, 51, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for _ in SENT_NUM_LIST:\n",
    "    print(f'{len(_)}개 : {_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] 인코딩<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 18)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "dataNP = np.array(SENT_NUM_LIST)\n",
    "dataNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인코딩\n",
    "ohEncoder = OneHotEncoder(sparse_output=False)\n",
    "SENT_VEC=ohEncoder.fit_transform(dataNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENT_VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
